{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgL7r5BOYLNtyhG3bYsHYN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushverma41/Mental-State-Prediction-using-NLP/blob/main/Code/DeBERTa-v3%20%2B%20Attention%20Pooling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5i5NCBswPeT"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ==========================================================\n",
        "!pip install transformers torch scikit-learn matplotlib seaborn joblib\n",
        "\n",
        "# ==========================================================\n",
        "# IMPORTS\n",
        "# ==========================================================\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel,\n",
        "    Trainer, TrainingArguments,\n",
        "    EarlyStoppingCallback, logging\n",
        ")\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# LOAD DATA\n",
        "# ==========================================================\n",
        "data_path = \"/content/drive/MyDrive/Mental State model/Data/Train_Data.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(\"üìä Dataset Loaded Successfully!\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "qZVSpvjjwt4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# LABEL ENCODING\n",
        "# ==========================================================\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['status'])\n",
        "num_labels = len(label_encoder.classes_)\n",
        "\n",
        "print(\"üß© Classes:\", label_encoder.classes_)\n",
        "print(\"Total Classes:\", num_labels)"
      ],
      "metadata": {
        "id": "MkFwxplwwzDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAIN-TEST SPLIT\n",
        "# ==========================================================\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['statement'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")"
      ],
      "metadata": {
        "id": "i-MaP8Hrw1vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TOKENIZATION\n",
        "# ==========================================================\n",
        "model_name = \"microsoft/deberta-v3-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "class MentalHealthDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = MentalHealthDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = MentalHealthDataset(val_texts, val_labels, tokenizer)\n"
      ],
      "metadata": {
        "id": "yJattXHdw3sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# MODEL DEFINITION (DeBERTa + Attention Pooling)\n",
        "# ==========================================================\n",
        "class DebertaAttentionClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels,\n",
        "                 embedding_dim=128, hidden_dim=128, output_dim=5, n_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.deberta = AutoModel.from_pretrained(model_name)\n",
        "        self.embedding_proj = nn.Linear(self.deberta.config.hidden_size, embedding_dim)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        layers = []\n",
        "        input_dim = embedding_dim\n",
        "        for _ in range(n_layers - 1):\n",
        "            layers += [\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ]\n",
        "            input_dim = hidden_dim\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        x = self.embedding_proj(last_hidden_state)\n",
        "        attn_weights = self.attention(x)\n",
        "        context_vector = torch.sum(attn_weights * x, dim=1)\n",
        "        logits = self.classifier(context_vector)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return {'loss': loss, 'logits': logits}"
      ],
      "metadata": {
        "id": "IncXuCm3w_kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAINING CONFIGURATION\n",
        "# ==========================================================\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "    f1 = f1_score(p.label_ids, preds, average='weighted')\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "model = DebertaAttentionClassifier(\n",
        "    model_name=model_name,\n",
        "    num_labels=num_labels,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    output_dim=5,\n",
        "    n_layers=2,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Mental State model/Model/DeBERTa-v3_Attention Pooling/\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,               # ‚úÖ 5 Epochs\n",
        "    weight_decay=0.01,                # ‚úÖ Regularization\n",
        "    load_best_model_at_end=True,      # ‚úÖ Prevent overfitting\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "logging.set_verbosity_info()"
      ],
      "metadata": {
        "id": "PTZ0TTVPxFno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAINING START\n",
        "# ==========================================================\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "wq-3lYShxJp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# SAVE TRAINED MODEL & LABEL ENCODER\n",
        "# ==========================================================\n",
        "save_path = \"/content/drive/MyDrive/Mental State model/Model/DeBERTa-v3_Attention Pooling/\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "joblib.dump(label_encoder, os.path.join(save_path, \"label_encoder.pkl\"))\n",
        "\n",
        "print(f\"\\n‚úÖ Model, tokenizer, and label encoder saved successfully at:\\n{save_path}\")\n"
      ],
      "metadata": {
        "id": "Ql2nocCXxLH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# MODEL EVALUATION & VISUALIZATION\n",
        "# ==========================================================\n",
        "img_path = \"/content/drive/MyDrive/Mental State model/Images/DeBERTa-v3_Attention Pooling/\"\n",
        "os.makedirs(img_path, exist_ok=True)\n",
        "\n",
        "train_metrics = trainer.evaluate(train_dataset)\n",
        "val_metrics = trainer.evaluate(val_dataset)\n",
        "\n",
        "train_acc = train_metrics[\"eval_accuracy\"]\n",
        "val_acc   = val_metrics[\"eval_accuracy\"]\n",
        "train_f1  = train_metrics[\"eval_f1\"]\n",
        "val_f1    = val_metrics[\"eval_f1\"]\n",
        "train_loss = train_metrics[\"eval_loss\"]\n",
        "val_loss   = val_metrics[\"eval_loss\"]\n",
        "\n",
        "print(\"\\nüìà Evaluation Results:\")\n",
        "print(f\"Training Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Training F1-score: {train_f1:.4f}, Validation F1-score: {val_f1:.4f}\")\n",
        "print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# ==== Accuracy & F1 Bar Chart ====\n",
        "plt.figure(figsize=(7,5))\n",
        "metrics = [\"Accuracy\", \"F1-score\"]\n",
        "train_values = [train_acc, train_f1]\n",
        "val_values = [val_acc, val_f1]\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, train_values, width, label=\"Training\", color=\"skyblue\")\n",
        "plt.bar(x + width/2, val_values, width, label=\"Validation\", color=\"lightgreen\")\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Training vs Validation Accuracy & F1-score\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_path, \"Accuracy_F1_Comparison.png\"))\n",
        "plt.show()\n",
        "\n",
        "# ==== Confusion Matrix & Heatmap ====\n",
        "predictions = trainer.predict(val_dataset)\n",
        "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix - Validation Data\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_path, \"Confusion_Matrix.png\"))\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìã Classification Report:\\n\")\n",
        "print(classification_report(true_labels, pred_labels, target_names=class_names))\n",
        "\n",
        "# ==== Loss Visualization ====\n",
        "if hasattr(trainer.state, \"log_history\"):\n",
        "    train_losses = [x[\"loss\"] for x in trainer.state.log_history if \"loss\" in x]\n",
        "    eval_losses  = [x[\"eval_loss\"] for x in trainer.state.log_history if \"eval_loss\" in x]\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.plot(eval_losses, label=\"Validation Loss\", marker='o')\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(img_path, \"Loss_Comparison.png\"))\n",
        "    plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ All visualizations saved in:\\n{img_path}\")"
      ],
      "metadata": {
        "id": "hPcpp4MjxOlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# SINGLE SENTENCE PREDICTION\n",
        "# ==========================================================\n",
        "def predict_sentence(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    preds = torch.argmax(outputs['logits'], dim=1)\n",
        "    return label_encoder.inverse_transform(preds.detach().numpy())[0]\n",
        "\n",
        "# Example\n",
        "print(\"\\nüîç Example Prediction:\")\n",
        "print(predict_sentence(\"I feel exhausted and mentally drained these days.\"))"
      ],
      "metadata": {
        "id": "Twe4kPpSxT8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "RKNs8VMFxWkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# IMPORTS\n",
        "# ==========================================================\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "import os\n",
        "from torch import nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "# ==========================================================\n",
        "# LOAD TRAINED MODEL, TOKENIZER, AND ENCODER\n",
        "# ==========================================================\n",
        "model_path = \"/content/drive/MyDrive/Mental State model/Model/DeBERTa-v3_Attention Pooling/\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "label_encoder = joblib.load(os.path.join(model_path, \"label_encoder.pkl\"))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==========================================================\n",
        "# MODEL DEFINITION (same architecture as training)\n",
        "# ==========================================================\n",
        "class DebertaAttentionClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels,\n",
        "                 embedding_dim=128, hidden_dim=128, output_dim=5, n_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.deberta = AutoModel.from_pretrained(model_name)\n",
        "        self.embedding_proj = nn.Linear(self.deberta.config.hidden_size, embedding_dim)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        layers = []\n",
        "        input_dim = embedding_dim\n",
        "        for _ in range(n_layers - 1):\n",
        "            layers += [\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ]\n",
        "            input_dim = hidden_dim\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        x = self.embedding_proj(last_hidden_state)\n",
        "        attn_weights = self.attention(x)\n",
        "        context_vector = torch.sum(attn_weights * x, dim=1)\n",
        "        logits = self.classifier(context_vector)\n",
        "        return logits\n",
        "\n",
        "# Load model\n",
        "model_name = \"microsoft/deberta-v3-base\"\n",
        "model = DebertaAttentionClassifier(model_name=model_name, num_labels=len(label_encoder.classes_))\n",
        "model.load_state_dict(torch.load(os.path.join(model_path, \"pytorch_model.bin\"), map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Model, tokenizer, and label encoder loaded successfully!\")\n",
        "\n",
        "# ==========================================================\n",
        "# LOAD TEST DATA\n",
        "# ==========================================================\n",
        "test_path = \"/content/drive/MyDrive/Mental State model/Data/Test_Data.csv\"\n",
        "test_df = pd.read_csv(test_path)\n",
        "print(f\"üìÇ Test Data Loaded: {test_df.shape[0]} samples\")\n",
        "\n",
        "# ==========================================================\n",
        "# TOKENIZE TEST DATA\n",
        "# ==========================================================\n",
        "class MentalHealthTestDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "test_dataset = MentalHealthTestDataset(test_df['statement'].tolist(), tokenizer)\n",
        "\n",
        "# ==========================================================\n",
        "# PREDICT ON TEST DATA\n",
        "# ==========================================================\n",
        "predictions = []\n",
        "batch_size = 16\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_dataset), batch_size):\n",
        "        batch = test_dataset[i:i+batch_size]\n",
        "        input_ids = torch.stack([x['input_ids'] for x in batch]).to(device)\n",
        "        attention_mask = torch.stack([x['attention_mask'] for x in batch]).to(device)\n",
        "\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "# Decode predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(predictions)\n",
        "test_df[\"Predicted_Status_DeBERTa_v3_Attention_Pooling\"] = predicted_labels\n",
        "\n",
        "# ==========================================================\n",
        "# SAVE OUTPUT TO CSV\n",
        "# ==========================================================\n",
        "output_path = \"/content/drive/MyDrive/Mental State model/Data/DeBERTa-v3_Attention_Pooling_Predictions.csv\"\n",
        "test_df.to_csv(output_path, index=False)\n",
        "print(f\"‚úÖ Predictions saved successfully to:\\n{output_path}\")\n",
        "\n",
        "# ==========================================================\n",
        "# SINGLE SENTENCE PREDICTION FUNCTION\n",
        "# ==========================================================\n",
        "def predict_sentence(sentence):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs)\n",
        "        pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
        "    return label_encoder.inverse_transform([pred])[0]\n",
        "\n",
        "# Example Usage\n",
        "example = \"I feel anxious and my mind won‚Äôt stop racing.\"\n",
        "predicted_class = predict_sentence(example)\n",
        "print(f\"\\nüîç Input: {example}\\nüß† Predicted Mental State: {predicted_class}\")\n"
      ],
      "metadata": {
        "id": "e-c7BEfDxXVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "GijT0FDdxdZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# IMPORTS\n",
        "# ==========================================================\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, log_loss\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from transformers import AutoModel\n",
        "import joblib\n",
        "\n",
        "# ==========================================================\n",
        "# PATH CONFIG\n",
        "# ==========================================================\n",
        "model_path = \"/content/drive/MyDrive/Mental State model/Model/DeBERTa-v3_Attention Pooling/\"\n",
        "img_path = \"/content/drive/MyDrive/Mental State model/Images/DeBERTa-v3_Attention Pooling/\"\n",
        "os.makedirs(img_path, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==========================================================\n",
        "# LOAD TOKENIZER, MODEL, LABEL ENCODER\n",
        "# ==========================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "label_encoder = joblib.load(os.path.join(model_path, \"label_encoder.pkl\"))\n",
        "\n",
        "# Model Architecture (same as used during training)\n",
        "class DebertaAttentionClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels,\n",
        "                 embedding_dim=128, hidden_dim=128, output_dim=5, n_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.deberta = AutoModel.from_pretrained(model_name)\n",
        "        self.embedding_proj = nn.Linear(self.deberta.config.hidden_size, embedding_dim)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        layers = []\n",
        "        input_dim = embedding_dim\n",
        "        for _ in range(n_layers - 1):\n",
        "            layers += [\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ]\n",
        "            input_dim = hidden_dim\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        x = self.embedding_proj(last_hidden_state)\n",
        "        attn_weights = self.attention(x)\n",
        "        context_vector = torch.sum(attn_weights * x, dim=1)\n",
        "        logits = self.classifier(context_vector)\n",
        "        return logits\n",
        "\n",
        "# Load model weights\n",
        "model_name = \"microsoft/deberta-v3-base\"\n",
        "model = DebertaAttentionClassifier(model_name, num_labels=len(label_encoder.classes_))\n",
        "model.load_state_dict(torch.load(os.path.join(model_path, \"pytorch_model.bin\"), map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# ==========================================================\n",
        "# LOAD TRAINING AND TEST DATA\n",
        "# ==========================================================\n",
        "train_data_path = \"/content/drive/MyDrive/Mental State model/Data/Train_Data.csv\"\n",
        "test_data_path = \"/content/drive/MyDrive/Mental State model/Data/Test_Data.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_data_path)\n",
        "test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "# Encode actual labels for test\n",
        "train_df[\"encoded\"] = label_encoder.transform(train_df[\"status\"])\n",
        "test_df[\"encoded\"] = label_encoder.transform(test_df[\"status\"])\n",
        "\n",
        "# ==========================================================\n",
        "# TOKENIZE & PREDICT FUNCTION\n",
        "# ==========================================================\n",
        "def get_predictions(df):\n",
        "    preds, probs = [], []\n",
        "    batch_size = 16\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(df), batch_size):\n",
        "            batch_texts = df[\"statement\"].iloc[i:i+batch_size].tolist()\n",
        "            enc = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True,\n",
        "                            padding=True, max_length=128).to(device)\n",
        "            logits = model(**enc)\n",
        "            pred_probs = torch.softmax(logits, dim=1)\n",
        "            pred_classes = torch.argmax(pred_probs, dim=1)\n",
        "            preds.extend(pred_classes.cpu().numpy())\n",
        "            probs.extend(pred_probs.cpu().numpy())\n",
        "    return np.array(preds), np.array(probs)\n",
        "\n",
        "# ==========================================================\n",
        "# GET TRAIN & TEST PREDICTIONS\n",
        "# ==========================================================\n",
        "train_preds, train_probs = get_predictions(train_df)\n",
        "test_preds, test_probs = get_predictions(test_df)\n",
        "\n",
        "# Decode predicted labels\n",
        "train_df[\"Predicted_Status\"] = label_encoder.inverse_transform(train_preds)\n",
        "test_df[\"Predicted_Status\"] = label_encoder.inverse_transform(test_preds)\n",
        "\n",
        "# ==========================================================\n",
        "# METRICS CALCULATION\n",
        "# ==========================================================\n",
        "train_acc = accuracy_score(train_df[\"encoded\"], train_preds)\n",
        "test_acc  = accuracy_score(test_df[\"encoded\"], test_preds)\n",
        "train_f1  = f1_score(train_df[\"encoded\"], train_preds, average=\"weighted\")\n",
        "test_f1   = f1_score(test_df[\"encoded\"], test_preds, average=\"weighted\")\n",
        "train_loss = log_loss(train_df[\"encoded\"], train_probs)\n",
        "test_loss  = log_loss(test_df[\"encoded\"], test_probs)\n",
        "\n",
        "print(\"\\nüìä Model Performance Summary:\")\n",
        "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Testing Accuracy:  {test_acc:.4f}\")\n",
        "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
        "print(f\"Testing F1-Score:  {test_f1:.4f}\")\n",
        "print(f\"Training Loss:     {train_loss:.4f}\")\n",
        "print(f\"Testing Loss:      {test_loss:.4f}\")\n",
        "\n",
        "# ==========================================================\n",
        "# 1Ô∏è‚É£ ACCURACY BAR CHART (TRAIN vs TEST)\n",
        "# ==========================================================\n",
        "plt.figure(figsize=(7,5))\n",
        "metrics = [\"Accuracy\", \"F1-Score\", \"Loss\"]\n",
        "train_values = [train_acc, train_f1, train_loss]\n",
        "test_values = [test_acc, test_f1, test_loss]\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, train_values, width, label=\"Training\", color=\"skyblue\")\n",
        "plt.bar(x + width/2, test_values, width, label=\"Testing\", color=\"lightgreen\")\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Training vs Testing Performance Comparison\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_path, \"Train_Test_Accuracy_Comparison.png\"))\n",
        "plt.show()\n",
        "\n",
        "# ==========================================================\n",
        "# 2Ô∏è‚É£ CONFUSION MATRIX (TRAINING)\n",
        "# ==========================================================\n",
        "train_cm = confusion_matrix(train_df[\"encoded\"], train_preds)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(train_cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Confusion Matrix - Training Data\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_path, \"Confusion_Matrix_Training.png\"))\n",
        "plt.show()\n",
        "\n",
        "# ==========================================================\n",
        "# 3Ô∏è‚É£ CONFUSION MATRIX (TESTING)\n",
        "# ==========================================================\n",
        "test_cm = confusion_matrix(test_df[\"encoded\"], test_preds)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(test_cm, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Confusion Matrix - Testing Data\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_path, \"Confusion_Matrix_Testing.png\"))\n",
        "plt.show()\n",
        "\n",
        "# ==========================================================\n",
        "# CLASSIFICATION REPORTS\n",
        "# ==========================================================\n",
        "print(\"\\nüìã Classification Report (Testing):\\n\")\n",
        "print(classification_report(test_df[\"encoded\"], test_preds, target_names=label_encoder.classes_))\n",
        "\n",
        "# ==========================================================\n",
        "# SAVE TEST PREDICTIONS TO CSV\n",
        "# ==========================================================\n",
        "output_path = \"/content/drive/MyDrive/Mental State model/Data/DeBERTa-v3_Attention_Pooling_Predictions.csv\"\n",
        "test_df.to_csv(output_path, index=False)\n",
        "print(f\"\\n‚úÖ Predictions with actual vs predicted labels saved at:\\n{output_path}\")\n",
        "\n",
        "# ==========================================================\n",
        "# 4Ô∏è‚É£ LOSS VISUALIZATION (TRAIN vs TEST)\n",
        "# ==========================================================\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.bar([\"Training Loss\", \"Testing Loss\"], [train_loss, test_loss],\n",
        "        color=[\"skyblue\", \"lightcoral\"])\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Testing Loss Comparison\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_path, \"Loss_Comparison.png\"))\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ All images saved at:\\n{img_path}\")\n"
      ],
      "metadata": {
        "id": "H4OuzLDOxeTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}