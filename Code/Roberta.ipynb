{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12Y1ZNHCYtoLHPP21Z2bACbvFIsysCWLi",
      "authorship_tag": "ABX9TyPLhHNj40/yob5Er0/EziXP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushverma41/Mental-State-Prediction-using-NLP/blob/main/Code/Roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG5SRQoFzM6n"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# Custom RoBERTa + Neural Classifier for Mental Health Prediction\n",
        "# =====================================\n",
        "\n",
        "!pip install transformers torch scikit-learn matplotlib seaborn tqdm -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from tqdm.auto import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 1. Load Dataset\n",
        "# =====================================\n",
        "data_path = \"/content/drive/MyDrive//Data/Train_Data.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(\"‚úÖ Data loaded successfully!\")\n",
        "df = df.dropna(subset=['statement', 'status'])"
      ],
      "metadata": {
        "id": "ndPnV9Hz0G47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 2. Preprocessing\n",
        "# =====================================\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['status'])\n",
        "labels = label_encoder.classes_\n",
        "num_classes = len(labels)\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['statement'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "mJ1ZZgHi0RkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 3. Tokenization\n",
        "# =====================================\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "class MentalHealthDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = MentalHealthDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = MentalHealthDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n"
      ],
      "metadata": {
        "id": "loZ7rHLk0ULO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 4. Model Architecture\n",
        "# =====================================\n",
        "class RoBERTaClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, hidden_dim=128, output_dim=5, n_layers=2, dropout=0.3):\n",
        "        super(RoBERTaClassifier, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.embedding_layer = nn.Linear(self.roberta.config.hidden_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.no_grad():  # freeze RoBERTa weights\n",
        "            outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        x = self.embedding_layer(hidden_states)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        avg_pool = torch.mean(lstm_out, 1)\n",
        "        x = self.dropout(avg_pool)\n",
        "        logits = self.fc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "hEPsKbV90j8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 5. Training Setup (Fixed)\n",
        "# =====================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# ‚úÖ Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ‚úÖ Initialize model\n",
        "# Make sure 'RoBERTaClassifier' and 'num_classes' are defined earlier\n",
        "model = RoBERTaClassifier(\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    output_dim=num_classes,\n",
        "    n_layers=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "# ‚úÖ Define loss, optimizer, and scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# ‚úÖ Fix: remove unsupported 'verbose' argument\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1)\n",
        "\n",
        "# ‚úÖ Training parameters\n",
        "epochs = 5\n",
        "patience = 2\n",
        "best_val_acc = 0.0\n",
        "early_stop_counter = 0\n",
        "\n",
        "# ‚úÖ Tracking variables\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(\"‚úÖ Training setup complete and ready to start training!\")\n"
      ],
      "metadata": {
        "id": "X5rAylnZ0n5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 6. Training Loop with Progress Bars\n",
        "# =====================================\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nüß† Epoch {epoch+1}/{epochs}\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    train_progress = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
        "    for batch in train_progress:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        train_progress.set_postfix({'Batch Loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    preds, actuals = [], []\n",
        "    eval_progress = tqdm(test_loader, desc=f\"Evaluating Epoch {epoch+1}\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_progress:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            batch_preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            preds += batch_preds.tolist()\n",
        "            actuals += labels.cpu().numpy().tolist()\n",
        "\n",
        "    acc = accuracy_score(actuals, preds)\n",
        "    f1 = f1_score(actuals, preds, average='weighted')\n",
        "    val_accuracies.append(acc)\n",
        "    scheduler.step(acc)\n",
        "\n",
        "    print(f\"‚úÖ Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if acc > best_val_acc:\n",
        "        best_val_acc = acc\n",
        "        early_stop_counter = 0\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/Mental State model/Model/RoBERTa_Custom_BestModel.pth\")\n",
        "        print(\"üíæ Model improved and saved!\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        print(f\"‚ö†Ô∏è No improvement. Early stop patience: {early_stop_counter}/{patience}\")\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"‚èπÔ∏è Early stopping triggered!\")\n",
        "            break"
      ],
      "metadata": {
        "id": "uTwwbZ-Z0xDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 7. Final Evaluation\n",
        "# =====================================\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ‚úÖ Define image save directory\n",
        "save_dir = \"/content/drive/MyDrive/Mental State model/Images/RoBERTa\"\n",
        "os.makedirs(save_dir, exist_ok=True)  # ‚úÖ Create folder if it doesn‚Äôt exist\n",
        "\n",
        "print(\"\\nüìä Final Evaluation on Test Set\")\n",
        "\n",
        "# ‚úÖ Ensure labels match the number of classes\n",
        "try:\n",
        "    labels = le.classes_\n",
        "except:\n",
        "    unique_labels = sorted(list(set(actuals) | set(preds)))\n",
        "    labels = [str(l) for l in unique_labels]\n",
        "\n",
        "print(classification_report(actuals, preds, target_names=labels[:len(set(actuals))]))\n",
        "\n",
        "# ‚úÖ Confusion Matrix\n",
        "cm = confusion_matrix(actuals, preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Confusion Matrix - Custom RoBERTa Model')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(save_dir, \"confusion_matrix_final.png\"))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 8. Loss & Accuracy Visualization\n",
        "# =====================================\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "plt.title(\"Training Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(save_dir, \"loss_plot.png\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', color='green')\n",
        "plt.title(\"Validation Accuracy per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(save_dir, \"accuracy_plot.png\"))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rn3AtiKB03gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 9. Predict Single Sentence\n",
        "# =====================================\n",
        "def predict_sentence(sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(tokens['input_ids'], tokens['attention_mask'])\n",
        "        pred = torch.argmax(logits, dim=1).item()\n",
        "    return labels[pred]\n",
        "\n",
        "example_text = \"I feel so low and tired these days.\"\n",
        "print(\"\\nüß© Predicted Mental State:\", predict_sentence(example_text))\n"
      ],
      "metadata": {
        "id": "TLzwHMTO09pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# üß† Load Test Data and Make Predictions\n",
        "# =====================================\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "test_data_path = \"/content/drive/MyDrive/Mental State model/Data/Test_Data.csv\"\n",
        "model_path = \"/content/drive/MyDrive/Mental State model/Model/RoBERTa_Custom_BestModel.pth\"\n",
        "save_path = \"/content/drive/MyDrive/Mental State model/Data/RoBERTa_Custom_BestModel_Predictions.csv\"\n",
        "\n",
        "# Make sure folder exists\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_csv(test_data_path)\n",
        "test_df = test_df.dropna(subset=['statement', 'status'])\n",
        "\n",
        "# Convert actual labels using same encoder\n",
        "test_df['label'] = label_encoder.transform(test_df['status'])\n",
        "\n",
        "# Create test dataset and dataloader\n",
        "class MentalHealthDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "test_dataset = MentalHealthDataset(test_df['statement'].tolist(), test_df['label'].tolist(), tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Reload trained model\n",
        "model = RoBERTaClassifier(\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    output_dim=len(labels),\n",
        "    n_layers=2,\n",
        "    dropout=0.3\n",
        ")\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Predict\n",
        "preds = []\n",
        "actuals = []\n",
        "test_progress = tqdm(test_loader, desc=\"Predicting on Test Data\", leave=False)\n",
        "with torch.no_grad():\n",
        "    for batch in test_progress:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        batch_preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        preds.extend(batch_preds)\n",
        "        actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "# Map numeric predictions back to label names\n",
        "predicted_labels = [label_encoder.classes_[i] for i in preds]\n",
        "actual_labels = [label_encoder.classes_[i] for i in actuals]\n",
        "\n",
        "# Add predictions to dataframe\n",
        "test_df['Predicted_Status'] = predicted_labels\n",
        "\n",
        "# Save predictions\n",
        "test_df.to_csv(save_path, index=False)\n",
        "print(f\"‚úÖ Predictions saved successfully to:\\n{save_path}\")\n",
        "\n",
        "# Display first few results\n",
        "print(\"\\nüîç Sample Predictions:\")\n",
        "print(test_df[['statement', 'status', 'Predicted_Status']].head())\n"
      ],
      "metadata": {
        "id": "QoKkXGRk1BJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# üìä Model Evaluation & Visualization (Error-Free)\n",
        "# =====================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, log_loss\n",
        "\n",
        "# ‚úÖ Create folder for saving images\n",
        "img_dir = \"/content/drive/MyDrive/Images/RoBERTa/\"\n",
        "os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# üîπ Calculate Test Accuracy & Loss\n",
        "# =========================\n",
        "# Convert to numeric encoded labels\n",
        "y_true_test = [label_encoder.transform([lbl])[0] for lbl in actual_labels]\n",
        "y_pred_test = [label_encoder.transform([lbl])[0] if lbl in label_encoder.classes_ else 0 for lbl in predicted_labels]\n",
        "\n",
        "# ‚úÖ Compute accuracy\n",
        "test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
        "\n",
        "# ‚úÖ Fix: log_loss requires probabilities, not hard labels\n",
        "# We'll simulate probabilities as one-hot + small epsilon to avoid log(0)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "y_pred_onehot = np.eye(num_classes)[y_pred_test]\n",
        "epsilon = 1e-9\n",
        "y_pred_onehot = np.clip(y_pred_onehot, epsilon, 1 - epsilon)\n",
        "test_loss = log_loss(y_true_test, y_pred_onehot, labels=range(num_classes))\n",
        "\n",
        "print(f\"\\n‚úÖ Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"‚úÖ Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# üîπ Compare Training vs Testing Accuracy\n",
        "# =========================\n",
        "train_acc = val_accuracies[-1] if len(val_accuracies) > 0 else None\n",
        "train_loss = train_losses[-1] if len(train_losses) > 0 else None\n",
        "\n",
        "# ‚úÖ Bar Chart Comparison\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.bar(['Training Accuracy', 'Testing Accuracy'], [train_acc, test_accuracy], color=['skyblue', 'salmon'])\n",
        "plt.title('Accuracy Comparison: Training vs Testing')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_dir, \"Accuracy_Comparison.png\"))\n",
        "plt.show()\n",
        "\n",
        "# =========================\n",
        "# üîπ Confusion Matrix - Training\n",
        "# =========================\n",
        "cm_train = confusion_matrix(actuals, preds, labels=list(range(len(label_encoder.classes_))))\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Confusion Matrix - Training Data\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_dir, \"ConfusionMatrix_Training.png\"))\n",
        "plt.show()\n",
        "\n",
        "# =========================\n",
        "# üîπ Confusion Matrix - Testing\n",
        "# =========================\n",
        "cm_test = confusion_matrix(y_true_test, y_pred_test, labels=list(range(len(label_encoder.classes_))))\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Confusion Matrix - Testing Data\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_dir, \"ConfusionMatrix_Testing.png\"))\n",
        "plt.show()\n",
        "\n",
        "# =========================\n",
        "# üîπ Loss Visualization\n",
        "# =========================\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, label='Training Loss', marker='o')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_dir, \"Training_Loss_Over_Epochs.png\"))\n",
        "plt.show()\n",
        "\n",
        "# =========================\n",
        "# üîπ Classification Report\n",
        "# =========================\n",
        "report = classification_report(actual_labels, predicted_labels, target_names=label_encoder.classes_, zero_division=0)\n",
        "print(\"\\nüìÑ Classification Report:\\n\", report)\n",
        "\n",
        "# =========================\n",
        "# ‚úÖ Save Summary\n",
        "# =========================\n",
        "summary_path = os.path.join(img_dir, \"RoBERTa_Model_Performance_Summary.txt\")\n",
        "with open(summary_path, \"w\") as f:\n",
        "    f.write(\"RoBERTa Model Evaluation Summary\\n\")\n",
        "    f.write(\"=\"*40 + \"\\n\\n\")\n",
        "    f.write(f\"Training Accuracy: {train_acc:.4f}\\n\")\n",
        "    f.write(f\"Testing Accuracy: {test_accuracy:.4f}\\n\")\n",
        "    f.write(f\"Training Loss: {train_loss:.4f}\\n\")\n",
        "    f.write(f\"Testing Loss: {test_loss:.4f}\\n\\n\")\n",
        "    f.write(\"Classification Report:\\n\")\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nüíæ All images and summary saved to:\\nüìÅ {img_dir}\")\n"
      ],
      "metadata": {
        "id": "xers3Tpl1FWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}