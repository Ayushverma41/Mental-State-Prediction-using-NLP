{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzLz+LK3ntqGdqBa6Imd9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushverma41/Mental-State-Prediction-using-NLP/blob/main/Code/RoBERTa%20%2B%20BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLa3z_ELu-Sv"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# STEP 1: Import Libraries\n",
        "# ==========================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 2: Load Dataset\n",
        "# ==========================================================\n",
        "data_path = \"/content/drive/MyDrive/Mental State model/Data/Train_Data.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "print(\"‚úÖ Dataset Loaded. Shape:\", df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "gXdwFHRkvAEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 3: Encode Labels\n",
        "# ==========================================================\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['status'])\n",
        "num_labels = len(label_encoder.classes_)\n",
        "print(\"Classes:\", label_encoder.classes_)"
      ],
      "metadata": {
        "id": "JO-KlcDuvBsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 4: Train-Test Split\n",
        "# ==========================================================\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['statement'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")"
      ],
      "metadata": {
        "id": "cuPAMsHKvJFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 5: Tokenization\n",
        "# ==========================================================\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n",
        "MAX_LEN = 128\n",
        "\n",
        "class MentalHealthDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = MentalHealthDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "val_dataset = MentalHealthDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "CT0goG5dvMwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 6: RoBERTa + BiLSTM Model (Custom Dimensions)\n",
        "# ==========================================================\n",
        "class RoBERTa_BiLSTM_Custom(nn.Module):\n",
        "    def __init__(self, output_dim=5, embedding_dim=128, hidden_dim=128, n_layers=2, dropout=0.3):\n",
        "        super(RoBERTa_BiLSTM_Custom, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.embedding_layer = nn.Linear(768, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        embeddings = self.embedding_layer(sequence_output)\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        avg_pool = torch.mean(lstm_out, dim=1)\n",
        "        max_pool, _ = torch.max(lstm_out, dim=1)\n",
        "        pooled = torch.cat((avg_pool, max_pool), dim=1)\n",
        "        out = self.dropout(pooled)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RoBERTa_BiLSTM_Custom(\n",
        "    output_dim=num_labels,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_layers=2,\n",
        "    dropout=0.3\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "O-s_CPUEvPiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 7: Training Setup (5 Epochs, Overfitting Control)\n",
        "# ==========================================================\n",
        "EPOCHS = 5\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "1KKD8T4EvSGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 8: Training & Evaluation Functions with Progress Bars\n",
        "# ==========================================================\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, epoch, total_epochs, max_grad_norm=1.0):\n",
        "    model.train()\n",
        "    losses, correct_predictions = [], 0\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{total_epochs} [Training]\", leave=False)\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{np.mean(losses):.4f}',\n",
        "            'Acc': f'{(correct_predictions.double() / len(data_loader.dataset)):.4f}'\n",
        "        })\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, epoch, total_epochs):\n",
        "    model.eval()\n",
        "    losses, correct_predictions = [], 0\n",
        "    all_preds, all_labels = [], []\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{total_epochs} [Validation]\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{np.mean(losses):.4f}',\n",
        "                'Acc': f'{(correct_predictions.double() / len(data_loader.dataset)):.4f}'\n",
        "            })\n",
        "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
        "    return acc, np.mean(losses), all_preds, all_labels"
      ],
      "metadata": {
        "id": "DI3Q5kxgvT6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 9: Training Loop with Early Stopping\n",
        "# ==========================================================\n",
        "best_val_loss = float('inf')\n",
        "patience = 2\n",
        "patience_counter = 0\n",
        "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n",
        "    train_acc, train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device, scheduler, epoch, EPOCHS)\n",
        "    val_acc, val_loss, val_preds, val_true = eval_model(model, val_loader, loss_fn, device, epoch, EPOCHS)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc.cpu().item())\n",
        "    val_accs.append(val_acc.cpu().item())\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        os.makedirs(\"/content/drive/MyDrive/Mental State model/Model/RoBERTa_BiLSTM/\", exist_ok=True)\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/Mental State model/Model/RoBERTa_BiLSTM/best_model.pt\")\n",
        "        print(\"‚úÖ Validation loss improved. Model checkpoint saved.\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"‚ö†Ô∏è No improvement. Early stopping patience: {patience_counter}/{patience}\")\n",
        "        if patience_counter >= patience:\n",
        "            print(\"üõë Early stopping triggered ‚Äî training stopped.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "dLYbP6rSvaRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 10: Final Evaluation, F1-score, Confusion Matrix & Plots\n",
        "# ==========================================================\n",
        "img_dir = \"/content/drive/MyDrive/Mental State model/Images/RoBERTa_BiLSTM/\"\n",
        "os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Mental State model/Model/RoBERTa_BiLSTM/best_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "val_acc, val_loss, val_preds, val_true = eval_model(model, val_loader, loss_fn, device, epoch=0, total_epochs=1)\n",
        "\n",
        "f1 = f1_score(val_true, val_preds, average='weighted')\n",
        "print(f\"\\nüìä Weighted F1-score: {f1:.4f}\")\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(classification_report(val_true, val_preds, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(val_true, val_preds)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Confusion Matrix - RoBERTa + BiLSTM\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_dir, \"confusion_matrix.png\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_dir, \"loss_curve.png\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, len(train_accs)+1), train_accs, label='Train Accuracy')\n",
        "plt.plot(range(1, len(val_accs)+1), val_accs, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_dir, \"accuracy_curve.png\"))\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ All plots saved in: {img_dir}\")"
      ],
      "metadata": {
        "id": "nR2MoPHxvfDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 11: Save Trained Model and Label Encoder\n",
        "# ==========================================================\n",
        "save_dir = \"/content/drive/MyDrive/Mental State model/Model/RoBERTa_BiLSTM/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(save_dir, \"RoBERTa_BiLSTM_model.pt\"))\n",
        "joblib.dump(label_encoder, os.path.join(save_dir, \"label_encoder.pkl\"))\n",
        "print(f\"‚úÖ Model and Label Encoder saved in: {save_dir}\")"
      ],
      "metadata": {
        "id": "SdTmoRSpvido"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTING**"
      ],
      "metadata": {
        "id": "xLoOptcnvmW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 1: Imports\n",
        "# ==========================================================\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer\n",
        "import joblib\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 2: Load Model, Tokenizer & Label Encoder\n",
        "# ==========================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Mental State model/Model/RoBERTa_BiLSTM/RoBERTa_BiLSTM_model.pt\"\n",
        "encoder_path = \"/content/drive/MyDrive/Mental State model/Model/RoBERTa_BiLSTM/label_encoder.pkl\"\n",
        "\n",
        "label_encoder = joblib.load(encoder_path)\n",
        "num_labels = len(label_encoder.classes_)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n",
        "\n",
        "# same architecture as before\n",
        "class RoBERTa_BiLSTM_Custom(torch.nn.Module):\n",
        "    def __init__(self, output_dim=5, embedding_dim=128, hidden_dim=128, n_layers=2, dropout=0.3):\n",
        "        super(RoBERTa_BiLSTM_Custom, self).__init__()\n",
        "        from transformers import RobertaModel\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.embedding_layer = torch.nn.Linear(768, embedding_dim)\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.fc = torch.nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        embeddings = self.embedding_layer(sequence_output)\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        avg_pool = torch.mean(lstm_out, dim=1)\n",
        "        max_pool, _ = torch.max(lstm_out, dim=1)\n",
        "        pooled = torch.cat((avg_pool, max_pool), dim=1)\n",
        "        out = self.dropout(pooled)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "# load trained weights\n",
        "model = RoBERTa_BiLSTM_Custom(output_dim=num_labels)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Model and encoder loaded successfully.\")\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 3: Load Test Data\n",
        "# ==========================================================\n",
        "test_path = \"/content/drive/MyDrive/Mental State model/Data/Test_Data.csv\"\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"‚úÖ Test data loaded. Shape: {test_df.shape}\")\n",
        "print(test_df.head())\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 4: Prepare Test Dataset\n",
        "# ==========================================================\n",
        "MAX_LEN = 128\n",
        "\n",
        "class MentalHealthDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "test_dataset = MentalHealthDataset(test_df['statement'].tolist(), tokenizer, MAX_LEN)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 5: Generate Predictions\n",
        "# ==========================================================\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Predicting on Test Data\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# decode labels\n",
        "predicted_labels = label_encoder.inverse_transform(all_preds)\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 6: Save Predictions\n",
        "# ==========================================================\n",
        "test_df['Predicted_Status_RoBERTa_BiLSTM'] = predicted_labels\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/Mental State model/Data/RoBERTa_BiLSTM_Predictions.csv\"\n",
        "test_df.to_csv(save_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Predictions saved to: {save_path}\")\n",
        "print(test_df.head())\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 7: Single Sentence Prediction Function\n",
        "# ==========================================================\n",
        "def predict_single_sentence(text):\n",
        "    model.eval()\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        pred = torch.argmax(outputs, dim=1).cpu().item()\n",
        "    return label_encoder.inverse_transform([pred])[0]\n",
        "\n",
        "# Example:\n",
        "sample_text = \"I feel so low and hopeless these days.\"\n",
        "predicted_class = predict_single_sentence(sample_text)\n",
        "print(f\"\\nüß† Input: {sample_text}\\n‚û°Ô∏è Predicted Mental State: {predicted_class}\")\n"
      ],
      "metadata": {
        "id": "JsonGWVhvmFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EVALUATION**"
      ],
      "metadata": {
        "id": "8lRDdV1RvskU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STEP 1: Imports\n",
        "# ==========================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 2: Define Constants\n",
        "# ==========================================================\n",
        "img_dir = \"/content/drive/MyDrive/Mental State model/Images/RoBERTa_BiLSTM/\"\n",
        "os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 3: Load Train & Test Data for Evaluation\n",
        "# ==========================================================\n",
        "train_path = \"/content/drive/MyDrive/Mental State model/Data/Train_Data.csv\"\n",
        "test_path = \"/content/drive/MyDrive/Mental State model/Data/Test_Data.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "train_labels = label_encoder.transform(train_df['status'])\n",
        "test_labels = label_encoder.transform(test_df['status'])\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 4: Prepare Datasets\n",
        "# ==========================================================\n",
        "class MentalHealthDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "MAX_LEN = 128\n",
        "train_dataset_eval = MentalHealthDataset(train_df['statement'], train_labels, tokenizer, MAX_LEN)\n",
        "test_dataset_eval = MentalHealthDataset(test_df['statement'], test_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader_eval = DataLoader(train_dataset_eval, batch_size=16, shuffle=False)\n",
        "test_loader_eval = DataLoader(test_dataset_eval, batch_size=16, shuffle=False)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 5: Evaluate Function (Acc & Loss)\n",
        "# ==========================================================\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    losses, preds_all, labels_all = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            preds_all.extend(preds.cpu().numpy())\n",
        "            labels_all.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels_all, preds_all)\n",
        "    avg_loss = np.mean(losses)\n",
        "    f1 = f1_score(labels_all, preds_all, average='weighted')\n",
        "    return acc, avg_loss, f1, labels_all, preds_all\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 6: Get Metrics for Train & Test\n",
        "# ==========================================================\n",
        "train_acc, train_loss, train_f1, y_train_true, y_train_pred = evaluate_model(model, train_loader_eval, device)\n",
        "test_acc, test_loss, test_f1, y_test_true, y_test_pred = evaluate_model(model, test_loader_eval, device)\n",
        "\n",
        "print(\"\\nüìä **Model Performance Summary**\")\n",
        "print(f\"Train Accuracy: {train_acc:.4f}, Train Loss: {train_loss:.4f}, F1: {train_f1:.4f}\")\n",
        "print(f\"Test  Accuracy: {test_acc:.4f}, Test  Loss: {test_loss:.4f}, F1: {test_f1:.4f}\")\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 7: Accuracy Comparison Bar Chart\n",
        "# ==========================================================\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.bar(['Training', 'Testing'], [train_acc, test_acc], color=['skyblue', 'lightgreen'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Testing Accuracy - RoBERTa + BiLSTM')\n",
        "for i, acc in enumerate([train_acc, test_acc]):\n",
        "    plt.text(i, acc + 0.01, f'{acc:.2f}', ha='center', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(img_dir, \"accuracy_comparison_bar.png\"))\n",
        "plt.show()\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 8: Confusion Matrix & Heatmaps\n",
        "# ==========================================================\n",
        "def plot_confusion(y_true, y_pred, dataset_type):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    plt.title(f'{dataset_type} Confusion Matrix - RoBERTa + BiLSTM')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.tight_layout()\n",
        "    filename = f\"{dataset_type.lower()}_confusion_matrix.png\"\n",
        "    plt.savefig(os.path.join(img_dir, filename))\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion(y_train_true, y_train_pred, \"Training\")\n",
        "plot_confusion(y_test_true, y_test_pred, \"Testing\")\n",
        "\n",
        "print(f\"‚úÖ All evaluation plots saved in: {img_dir}\")\n"
      ],
      "metadata": {
        "id": "WwpaYFgivtev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}